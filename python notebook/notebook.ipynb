{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "3ca4229c4c454894a9447f3b970c5ea0",
    "deepnote_cell_type": "code",
    "execution_context_id": "c91d6ab7-fd65-4dce-a171-9d4f80f94d82",
    "execution_millis": 3901,
    "execution_start": 1731834422335,
    "source_hash": "53c7a4c8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8a5ed620d8f6482b96d29d0431a77363",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# RecSys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "461b58cbfd524606a0e44f6ebf584a4b",
    "deepnote_cell_type": "code",
    "execution_context_id": "c91d6ab7-fd65-4dce-a171-9d4f80f94d82",
    "execution_millis": 1092,
    "execution_start": 1731834427425,
    "source_hash": "5e02f7bc"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/table_interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>courseID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962100</th>\n",
       "      <td>6040</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962101</th>\n",
       "      <td>6040</td>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962102</th>\n",
       "      <td>6040</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962103</th>\n",
       "      <td>6040</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962104</th>\n",
       "      <td>6040</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>962105 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userID  courseID  rating\n",
       "0            1      1193       5\n",
       "1            1       661       3\n",
       "2            1       914       3\n",
       "3            1      3408       4\n",
       "4            1      2355       5\n",
       "...        ...       ...     ...\n",
       "962100    6040      1091       1\n",
       "962101    6040      1094       5\n",
       "962102    6040       562       5\n",
       "962103    6040      1096       4\n",
       "962104    6040      1097       4\n",
       "\n",
       "[962105 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['userID', 'courseID']]\n",
    "y = df[['rating']]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((X_train['userID'].values, X_train['courseID'].values), y_train.values)\n",
    ")\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((X_valid['userID'].values, X_valid['courseID'].values), y_valid.values)\n",
    ")\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "dl_train = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)\n",
    "dl_valid = valid_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "80a8619124eb4f13ac4d80bf2875ee89",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "ea07640ede7645a1bfa0fede16f194ff",
    "deepnote_cell_type": "code",
    "execution_context_id": "92a35e28-cd5f-4a3e-ba22-fe76791f59b7",
    "execution_millis": 3,
    "execution_start": 1731834097362,
    "source_hash": "2e5bc027"
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package='Custom')\n",
    "class MF(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, emb_dim, init=True, bias=True, sigmoid=True, **kwargs):\n",
    "        super(MF, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.init = init\n",
    "        self.bias = bias\n",
    "        self.sigmoid = sigmoid\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_emb = tf.keras.layers.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = tf.keras.layers.Embedding(num_items, emb_dim)\n",
    "        \n",
    "        if init:\n",
    "            self.user_emb.embeddings_initializer = tf.keras.initializers.RandomUniform(0., 0.05)\n",
    "            self.item_emb.embeddings_initializer = tf.keras.initializers.RandomUniform(0., 0.05)\n",
    "        \n",
    "        if bias:\n",
    "            self.user_bias = self.add_weight(name=\"user_bias\", shape=(num_users,), initializer=\"zeros\", trainable=True)\n",
    "            self.item_bias = self.add_weight(name=\"item_bias\", shape=(num_items,), initializer=\"zeros\", trainable=True)\n",
    "            self.offset = self.add_weight(name=\"offset\", shape=(), initializer=\"zeros\", trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        user, item = inputs\n",
    "\n",
    "        # Look up embeddings\n",
    "        user_emb = self.user_emb(user)\n",
    "        item_emb = self.item_emb(item)\n",
    "\n",
    "        # Compute dot product\n",
    "        element_product = tf.reduce_sum(user_emb * item_emb, axis=1)\n",
    "\n",
    "        if self.bias:\n",
    "            # Add biases\n",
    "            user_b = tf.gather(self.user_bias, user)\n",
    "            item_b = tf.gather(self.item_bias, item)\n",
    "            element_product += user_b + item_b + self.offset\n",
    "\n",
    "        if self.sigmoid:\n",
    "            return self.sigmoid_range(element_product, low=0, high=5.5)\n",
    "\n",
    "        return element_product\n",
    "\n",
    "    def predict(self, user_id, course_id, k=10):\n",
    "        tensor_user = tf.convert_to_tensor([user_id] * len(course_id), dtype=tf.int32)\n",
    "        tensor_course = tf.convert_to_tensor(course_id, dtype=tf.int32)\n",
    "    \n",
    "        pred = self.call((tensor_user, tensor_course))\n",
    "        rank = tf.argsort(pred, direction='DESCENDING')[:k].numpy().flatten()\n",
    "        rec_id = tf.gather(tensor_course, rank)\n",
    "    \n",
    "        return rec_id.numpy().tolist()\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_range(x, low=0, high=5.5):\n",
    "        return tf.sigmoid(x) * (high - low) + low\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MF, self).get_config()\n",
    "        config.update({\n",
    "            \"num_users\": self.num_users,\n",
    "            \"num_items\": self.num_items,\n",
    "            \"emb_dim\": self.emb_dim,\n",
    "            \"init\": self.init,\n",
    "            \"bias\": self.bias,\n",
    "            \"sigmoid\": self.sigmoid,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7ce6135dd671499d821252b42a398f09",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "319213f231154114afc4ab02bb7f9a7e",
    "deepnote_cell_type": "code",
    "execution_context_id": "92a35e28-cd5f-4a3e-ba22-fe76791f59b7",
    "execution_millis": 169,
    "execution_start": 1731833554002,
    "source_hash": "5e39f32e"
   },
   "outputs": [],
   "source": [
    "n_users = len(df['userID'].unique()) + 1\n",
    "n_items = 3712 + 1 # len(df['courseID'].unique()) + 1\n",
    "\n",
    "model = MF(n_users, n_items, emb_dim=64,\n",
    "           init=False,\n",
    "           bias=True,\n",
    "           sigmoid=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "17e4cc14689646c682dbe54a600e217b",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 24ms/step - loss: 1.5584 - mean_absolute_error: 1.0395 - val_loss: 0.9307 - val_mean_absolute_error: 0.7815\n",
      "Epoch 2/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 0.8645 - mean_absolute_error: 0.7466 - val_loss: 0.7996 - val_mean_absolute_error: 0.7091\n",
      "Epoch 3/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - loss: 0.7446 - mean_absolute_error: 0.6838 - val_loss: 0.7633 - val_mean_absolute_error: 0.6899\n",
      "Epoch 4/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - loss: 0.6763 - mean_absolute_error: 0.6499 - val_loss: 0.7430 - val_mean_absolute_error: 0.6795\n",
      "Epoch 5/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - loss: 0.6135 - mean_absolute_error: 0.6184 - val_loss: 0.7334 - val_mean_absolute_error: 0.6740\n",
      "Epoch 6/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - loss: 0.5599 - mean_absolute_error: 0.5901 - val_loss: 0.7322 - val_mean_absolute_error: 0.6723\n",
      "Epoch 7/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - loss: 0.5076 - mean_absolute_error: 0.5614 - val_loss: 0.7375 - val_mean_absolute_error: 0.6738\n",
      "Epoch 8/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - loss: 0.4644 - mean_absolute_error: 0.5361 - val_loss: 0.7470 - val_mean_absolute_error: 0.6771\n",
      "Epoch 9/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 0.4283 - mean_absolute_error: 0.5141 - val_loss: 0.7595 - val_mean_absolute_error: 0.6819\n",
      "Epoch 10/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 0.3957 - mean_absolute_error: 0.4933 - val_loss: 0.7741 - val_mean_absolute_error: 0.6874\n",
      "Epoch 11/100\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 0.3701 - mean_absolute_error: 0.4762 - val_loss: 0.7892 - val_mean_absolute_error: 0.6929\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17fae0ff710>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    dl_train,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_data=dl_valid,\n",
    "    shuffle=True,\n",
    "    initial_epoch=0,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mf_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mf_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">386,624</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">237,632</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │         \u001b[38;5;34m386,624\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_15 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │         \u001b[38;5;34m237,632\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,902,035</span> (7.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,902,035\u001b[0m (7.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">634,011</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m634,011\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,268,024</span> (4.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,268,024\u001b[0m (4.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save('./model/MF_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Fery\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fery\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:730: UserWarning: Model 'mf_7' had a build config, but the model cannot be built automatically in `build_from_config(config)`. You should implement `def build_from_config(self, config)`, and you might also want to implement the method  that generates the config at saving time, `def get_build_config(self)`. The method `build_from_config()` is meant to create the state of the model (i.e. its variables) upon deserialization.\n",
      "  instance.build_from_config(build_config)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./model/MF_model.keras', custom_objects={'MF': MF})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d470f022179549eda09c6d7c0633e4ce",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Fery\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# encoder = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('./dataset/table_courses_info.csv')\n",
    "desc_embedding = encoder.encode(db['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(encoder, skillset, k=10, threshold=None):\n",
    "    tensor_skillset = encoder(skillset)\n",
    "    tensor_course = desc_embedding\n",
    "    tensor_skillset = tf.nn.l2_normalize(tensor_skillset, axis=-1)\n",
    "    tensor_course = tf.nn.l2_normalize(tensor_course, axis=-1)\n",
    "\n",
    "    cos_sim = tf.squeeze(tf.matmul(tensor_course, tensor_skillset, transpose_b=True))\n",
    "\n",
    "    if threshold is not None:\n",
    "        indices = tf.where(cos_sim >= threshold).numpy().flatten()\n",
    "        top_idx = indices[tf.argsort(tf.gather(cos_sim, indices), direction='DESCENDING').numpy()]\n",
    "    else:\n",
    "        top_idx = tf.argsort(cos_sim, axis=0, direction='DESCENDING')[:k].numpy().flatten()\n",
    "\n",
    "    rec_id = db.iloc[top_idx]['ID'].tolist()\n",
    "    rec_name = db.iloc[top_idx]['Title'].tolist()\n",
    "\n",
    "    return rec_id, rec_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender(user_id, skillset, encoder, model, n=50, k=10):\n",
    "    if user_id > model.user_emb.input_dim - 1:\n",
    "        user = 0\n",
    "    else:\n",
    "        user = user_id\n",
    "\n",
    "    course_ids, course_names = vector_search(encoder, skillset, k=n)\n",
    "    course = course_ids\n",
    "\n",
    "    pred = model.predict(user, course, k=k)\n",
    "    rec = db[db['ID'].isin(pred)]\n",
    "\n",
    "    return rec['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[364, 531, 1210, 1487, 1544, 1876, 1958, 2406, 2539]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iUser = 1\n",
    "iSkill = ['Math, Machine Learning, Computer Science', 'Python']\n",
    "recommender(iUser, iSkill, encoder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def new_user_update(new_user_id, preferences, encoder, threshold=0.5):\n",
    "    interactions = df.pivot_table(\n",
    "        index='user_id',\n",
    "        columns='course_id', \n",
    "        values='rating', \n",
    "        fill_value=0\n",
    "    )\n",
    "    course_ids, _ = vector_search(encoder, preferences, threshold=threshold)\n",
    "    \n",
    "    rating_new_user = np.zeros(len(df['course_encode'].unique()))\n",
    "    indices = interactions.columns.get_indexer(course_ids)\n",
    "    rating_new_user[indices] = 4\n",
    "\n",
    "    interactions.loc[new_user_id] = rating_new_user\n",
    "    new_df = interactions.reset_index().melt(\n",
    "        id_vars='user_id',\n",
    "        var_name='course_id',\n",
    "        value_name='rating'\n",
    "    )\n",
    "    new_df = new_df[new_df['rating'] != 0].reset_index(drop=True)\n",
    "\n",
    "    inv_user_map = df.groupby('user_id')['user_encode'].first().reset_index().set_index('user_encode').to_dict()['user_id']\n",
    "    user_map = {v: k for k, v in inv_user_map.items()}\n",
    "    user_map[new_user_id] = max(user_map.values()) + 1\n",
    "    \n",
    "    inv_course_map = df.groupby('course_id')['course_encode'].first().reset_index().set_index('course_encode').to_dict()['course_id']\n",
    "    course_map = {v: k for k, v in inv_course_map.items()}\n",
    "    course_map[new_user_id] = max(course_map.values()) + 1\n",
    "\n",
    "    new_df['user_encode'] = new_df['user_id'].map(user_map)\n",
    "    new_df['course_encode'] = new_df['course_id'].map(course_map)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def recommender(input_user, input_skillset, encoder, model, n=50, k=10):\n",
    "    user_encode = df[df['user_id'] == input_user]['user_encode'].values[0]\n",
    "    if user_encode > model.user_emb.input_dim - 1:\n",
    "        interactions = df.pivot_table(\n",
    "            index='user_id', \n",
    "            columns='course_id', \n",
    "            values='rating', \n",
    "            fill_value=0\n",
    "        )\n",
    "        new_interaction = interactions.loc[input_user]\n",
    "        exist_interaction = interactions.drop(input_user)\n",
    "        \n",
    "        similarity = np.matmul(exist_interaction.values, new_interaction.values)\n",
    "        position = tf.argsort(similarity, direction='DESCENDING').numpy()[0]\n",
    "        user_sim = exist_interaction.index[position]\n",
    "        user = df[df['user_id'] == user_sim]['user_encode'].unique().item()\n",
    "    else:\n",
    "        user = df[df['user_id'] == input_user]['user_encode'].unique().item()\n",
    "\n",
    "    course_ids, course_names = vector_search(encoder, input_skillset, k=n)\n",
    "    course = df[df['course_id'].isin(course_ids)]['course_encode'].unique().tolist()\n",
    "\n",
    "    pred = model.predict(user, course, k=k)\n",
    "    pred_id = df[df['course_encode'].isin(pred)]['course_id'].unique().tolist()\n",
    "\n",
    "    rec = db[db['course_id'].isin(pred_id)][['course_id', 'Course Name', 'Course URL']]\n",
    "\n",
    "    response = {\n",
    "        idx: {\n",
    "            'course_id': row['course_id'],\n",
    "            'course_name': row['Course Name'],\n",
    "            'course_url': row['Course URL'],\n",
    "        }\n",
    "        for idx, row in rec.iterrows()\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# --"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "9d0f6a5e67714c11aec770d1950374ce",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
